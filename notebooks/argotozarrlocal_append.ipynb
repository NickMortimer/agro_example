{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting argo data into Zarr format\n",
    "\n",
    "\n",
    "Get some data, use rsync to get all the CSIRO profile data\n",
    "\n",
    "rsync -avzh --delete  --include=\"*/\" --include '*prof.nc' --exclude=\"*\" vdmzrs.ifremer.fr::argo/csiro .\n",
    "\n",
    "xarray added appending\n",
    "\n",
    "https://github.com/pydata/xarray/pull/2706\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch1/mor582/miniconda3/envs/pyAODN/lib/python3.6/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/scratch1/mor582/miniconda3/envs/pyAODN/lib/python3.6/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "from distributed import Client\n",
    "from tqdm import tqdm_notebook\n",
    "import numcodecs\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the path to the argo files\n",
    "argopath = '/scratch1/mor582/argo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a csv file with the catalogue in it\n",
    "files = glob.glob(argopath +'**/*_prof.nc',recursive=True)\n",
    "pd.DataFrame(files,columns=['files']).sort_values('files').to_csv(argopath+'catalogue.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the catalog (this is faster than doing glob glob every run)\n",
    "dffiles = pd.read_csv(argopath+'catalogue.csv')\n",
    "files = list(dffiles.files)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch1/mor582/miniconda3/envs/pyAODN/lib/python3.6/site-packages/dask_jobqueue/config.py:12: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6c0dc7409f429284668a8ba91ee850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>SLURMCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from distributed import LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://152.83.81.91:45009\n",
       "  <li><b>Dashboard: </b><a href='http://152.83.81.91:8787/status' target='_blank'>http://152.83.81.91:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>36</li>\n",
       "  <li><b>Cores: </b>72</li>\n",
       "  <li><b>Memory: </b>576.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://152.83.81.91:45009' processes=36 cores=72>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "data_types ={'CONFIG_MISSION_NUMBER':'float32','CYCLE_NUMBER':'float32','DATA_CENTRE':'|U2','DATA_MODE':'|U1',\n",
    "             'DATA_STATE_INDICATOR':'|U4','DC_REFERENCE':'|U32','DIRECTION':'|U1','FIRMWARE_VERSION':'|U32',\n",
    "             'FLOAT_SERIAL_NO':'|U32','JULD':'float32','JULD_LOCATION':'float32','JULD_QC':'|U1','LATITUDE':'float32',\n",
    "             'LONGITUDE':'float32','PI_NAME':'|U64','PLATFORM_NUMBER':'|U8','PLATFORM_TYPE':'|U32','POSITIONING_SYSTEM':'|U8',\n",
    "             'POSITION_QC':'|U1','PRES':'float32','PRES_ADJUSTED':'float32','PRES_ADJUSTED_ERROR':'float32',\n",
    "             'PRES_ADJUSTED_QC':'|U1','PRES_QC':'|U1','PROFILE_PRES_QC':'|U1','PROFILE_PSAL_QC':'|U1','PROFILE_TEMP_QC':'|U1',\n",
    "             'PROJECT_NAME':'|U64','PSAL':'float32','PSAL_ADJUSTED':'float32','PSAL_ADJUSTED_ERROR':'float32',\n",
    "             'PSAL_ADJUSTED_QC':'|U1','PSAL_QC':'|U1','TEMP':'float32','TEMP_ADJUSTED':'float32','TEMP_ADJUSTED_ERROR':'float32',\n",
    "             'TEMP_ADJUSTED_QC':'|U1','TEMP_QC':'|U1','VERTICAL_SAMPLING_SCHEME':'|U256','WMO_INST_TYPE':'|U4'}\n",
    "\n",
    "data_levels =['PRES','PRES_ADJUSTED','PRES_ADJUSTED_ERROR','PRES_ADJUSTED_QC','PRES_QC','PSAL','PSAL_ADJUSTED',\n",
    "              'PSAL_ADJUSTED_ERROR','PSAL_ADJUSTED_QC','PSAL_QC','TEMP','TEMP_ADJUSTED','TEMP_ADJUSTED_ERROR',\n",
    "              'TEMP_ADJUSTED_QC','TEMP_QC']\n",
    "\n",
    "def process_mf(dsinput,levels,data_types=data_types,data_levels=data_levels):\n",
    "    ds = xr.Dataset()\n",
    "    #pad =xr.DataArray(np.ones((levels-len( dsinput.N_LEVELS),len(dsinput.N_PROF))) *np.nan,dims={'N_LEVELS','N_PROF'})\n",
    "    #pad_qc = xr.DataArray(np.chararray((levels-len( dsinput.N_LEVELS),len(dsinput.N_PROF))),dims={'N_LEVELS','N_PROF'})\n",
    "    dims =('N_PROF','N_LEVELS')\n",
    "    pading =xr.DataArray(np.ones((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))) *np.nan,dims=dims)\n",
    "    pad_qc = xr.DataArray(np.chararray((len(dsinput.N_PROF),levels-len( dsinput.N_LEVELS))),dims=dims)\n",
    "    pad_qc[:] = b' '\n",
    "    for varname in data_types.keys():\n",
    "        if varname in dsinput.data_vars:\n",
    "            da = dsinput[varname]\n",
    "            if 'N_LEVELS' in da.dims:   \n",
    "                if varname in dsinput.data_vars:\n",
    "                    if varname.endswith('QC'):\n",
    "                        da = xr.concat([dsinput[varname],pad_qc],dim='N_LEVELS').astype(data_types[varname])\n",
    "                    else:\n",
    "                        da = xr.concat([dsinput[varname],pading],dim='N_LEVELS').astype(data_types[varname])\n",
    "            else:\n",
    "                da = dsinput[varname].astype(data_types[varname])\n",
    "        else:\n",
    "            if varname in data_levels:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones((len(dsinput.N_PROF),levels), dtype='float32')*np.nan , name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF),levels))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF','N_LEVELS'])\n",
    "            else:\n",
    "                if data_types[varname]=='float32':\n",
    "                    da = xr.DataArray(np.ones(len(dsinput.N_PROF), dtype=\"float32\")*np.nan , name=varname, dims=['N_PROF'])\n",
    "                else:\n",
    "                    p=np.chararray((len(dsinput.N_PROF)))\n",
    "                    p[:]=b'0'\n",
    "                    da = xr.DataArray(p.astype(data_types[varname]), name=varname, dims=['N_PROF'])\n",
    "        if not ('HISTORY' in varname) and ('N_CALIB' not in da.dims) and ('N_PARAM' not in da.dims) and  ('N_PROF' in da.dims):\n",
    "                ds[varname]= da\n",
    "    return ds.chunk({'N_LEVELS':3000})\n",
    "   \n",
    "preproc = partial(process_mf,levels=3000)\n",
    "\n",
    "@dask.delayed\n",
    "def process_float(file):\n",
    "    data = preproc(xr.open_dataset(file, chunks={'N_LEVELS':100}))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87889e87d59e4d87b9c063a6642897bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0\n",
      "Finished concatenating dataset\n",
      "Appending Done!\n",
      "Processing 200\n",
      "Finished concatenating dataset\n",
      "Appending Done!\n",
      "Processing 400\n",
      "Finished concatenating dataset\n",
      "Appending Done!\n",
      "Processing 600\n",
      "Finished concatenating dataset\n",
      "Appending Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def process_float(file):\n",
    "    data = preproc(xr.open_dataset(file, chunks={'N_LEVELS':100}))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d =[]\n",
    "\n",
    "start = 0\n",
    "incr = 200\n",
    "stop = len(files)\n",
    "ranges = list(range(start, stop, incr))\n",
    "for i in  tqdm_notebook(ranges):\n",
    "    print(f'Processing {i}')\n",
    "    d = []\n",
    "    for file in files[i:i+incr]:\n",
    "        d.append(process_float(file))\n",
    "\n",
    "    results = dask.compute(*d)\n",
    "\n",
    "    t = xr.concat(results,dim='N_PROF', coords='minimal')    \n",
    "    t = t.chunk({'N_PROF':10000,'N_LEVELS':3000})\n",
    "    print(f'Finished concatenating dataset')\n",
    "    \n",
    "    numcodecs.blosc.use_threads = False\n",
    "    synchronizer = zarr.ProcessSynchronizer(argopath+'argodask2.sync')\n",
    "    #compressor = zarr.Blosc(cname='zstd', clevel=3, shuffle=2)\n",
    "    zarr_path =  argopath+'csiro.zarr'\n",
    "    #encoding = {vname: {'compressor': compressor} for vname in t.variables}\n",
    "    d = t.to_zarr(zarr_path,mode='a',synchronizer=synchronizer,compute=True,append_dim='N_PROF')\n",
    "    print('Appending Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
